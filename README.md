# **Learning-from-Data-Caltech: Solutions**

This repository contains the source code to my solutions of course homework. 

Course website: https://work.caltech.edu/telecourse.html

## Homework 1

https://work.caltech.edu/homework/hw1.pdf

- [Week 1&2](/HW1/), Hoeffding's inequality and Perceptron.

## Homework 2

https://work.caltech.edu/homework/hw2.pdf

- [Week 3&4](/HW2/), Linear Regression and Linear Classification, also noise.

## Homework 3

https://work.caltech.edu/homework/hw3.pdf

- [Week 5&6](/HW3/), A bit difficult, it includes a introduction of Number of Hypotheses and dichotomy, growth function and break point. The homework is focus on calculate break point and growth function.
- Here is a note: https://zhuanlan.zhihu.com/p/41109051

## Homework 4

https://work.caltech.edu/homework/hw4.pdf

- [Week 7&8](/HW4/), VC Dimension and Bias-Variance Trade-off, an important portion is the Learning Curve. I have included another solution from others with good math answers.

## Homework 5

https://work.caltech.edu/homework/hw5.pdf

- [Week 9&10](/HW5/), Logistic Regression and a brief introduction of neural network. Also gradient desent method. In this assigment a Logistic model is built.
- Here is a good learning note: https://chih-sheng-huang821.medium.com/機器學習-基礎數學-二-梯度下降法-gradient-descent-406e1fd001f .

## Homework 6

https://work.caltech.edu/homework/hw6.pdf

- [Week 11&12](/HW6/), this week focus on reason of overfitting and regularization methods, weight decay.

## Homework 7

https://work.caltech.edu/homework/hw7.pdf

- [Week 13&14](/HW7/), Validation methods and Support Vector Machine, important!

## Homework 8

https://work.caltech.edu/homework/hw8.pdf

- [Week 15&16](/HW8/), Soft-Margin Support Vector Machine and Kernel tricks, important! And RBF, which is a effective tool.
- Here is a note includes the nature of Support Vector Machine: https://blog.csdn.net/cyril_ki/article/details/107601621

## Final

https://work.caltech.edu/homework/final.pdf

- [Week 17&18](/FInal/), Complete the final assignment, built RBF model, both in regular form (Lloyd + pseudo-inverse) and in kernel form (using the RBF kernel in hardmargin SVM. Finish this course!

If you spot a mistake, I'll be happy to hear about it! Thanks!
